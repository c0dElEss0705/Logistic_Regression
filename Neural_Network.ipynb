{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45561763-f28a-4a15-96c0-a99b264039c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d731be3-b6ed-4681-be74-1518ef3d2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8ad45-cac6-4e0c-9bcf-2ab9baf7e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = fetch_openml(\"mnist_784\")\n",
    "x = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "x /= 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "num_labels = 10\n",
    "examples = y.shape[0]\n",
    "y_new = one_hot(y.astype('int32'), num_labels)\n",
    "\n",
    "# Split, reshape, shuffle\n",
    "train_size = 60000\n",
    "test_size = x.shape[0] - train_size\n",
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y_new[:train_size], y_new[train_size:]\n",
    "shuffle_index = np.random.permutation(train_size)\n",
    "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643941b-94f2-4492-8dec-a4bc514890d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Test data: {} {}\".format(x_test.shape, y_test.shape))\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d81a35-5527-44a1-8fa7-ba28d284d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, sizes, activation='sigmoid'):\n",
    "    self.sizes = sizes\n",
    "\n",
    "    # Choose activation function\n",
    "    if activation == 'relu':\n",
    "        self.activation = self.relu\n",
    "    elif activation == 'sigmoid':\n",
    "        self.activation = self.sigmoid\n",
    "\n",
    "    # Save all weights\n",
    "    self.params = self.initialize()\n",
    "    # Save all intermediate values, i.e. activations\n",
    "    self.cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b293c-78b6-47d8-b7d6-ae567d876b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(self):\n",
    "    # Number of nodes in each layer\n",
    "    input_layer=self.sizes[0]\n",
    "    hidden_1=self.sizes[1]\n",
    "    hidden_2=self.sizes[2]\n",
    "    output_layer=self.sizes[3]\n",
    "\n",
    "    params = {\n",
    "        'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1./hidden_1),\n",
    "        'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1./hidden_2),\n",
    "        'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1./output_layer)\n",
    "    }\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d39ea-222a-473b-9968-c3295efb8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(self, x):\n",
    "    self.cache[\"X\"] = x\n",
    "    self.cache[\"Z1\"] = np.matmul(self.params[\"W1\"], self.cache[\"X\"].T) +\\\n",
    "                                                        self.params[\"b1\"]\n",
    "    self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
    "    self.cache[\"Z2\"] = np.matmul(self.params[\"W2\"], self.cache[\"A1\"]) +\\\n",
    "                                                        self.params[\"b2\"]\n",
    "    self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"])\n",
    "    return self.cache[\"A2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a7eee-10e3-4850-aaee-580b43e578a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(self, x, derivative=False):\n",
    "    if derivative:\n",
    "        x = np.where(x < 0, 0, x)\n",
    "        x = np.where(x >= 0, 1, x)\n",
    "        return x\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(self, x, derivative=False):\n",
    "    if derivative:\n",
    "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def softmax(self, x):\n",
    "    # Numerically stable with large exponentials\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56568b-068f-46b1-9a4a-3faa05ea2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(self, y, output):\n",
    "    current_batch_size = y.shape[0]\n",
    "\n",
    "    dZ2 = output - y.T\n",
    "    dW2 = (1./current_batch_size) * np.matmul(dZ2, self.cache[\"A1\"].T)\n",
    "    db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(self.params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * self.activation(self.cache[\"Z1\"], derivative=True)\n",
    "    dW1 = (1./current_batch_size) * np.matmul(dZ1, self.cache[\"X\"])\n",
    "    db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "    return self.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba3a93-8f29-44ff-aec6-c3cf3ee7aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, x_train, y_train, x_test, y_test):\n",
    "    for i in range(self.epochs):\n",
    "        # Shuffle\n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = y_train[permutation]\n",
    "\n",
    "        for j in range(num_batches):\n",
    "            # Batch\n",
    "            begin = j * self.batch_size\n",
    "            end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "            x = x_train_shuffled[begin:end]\n",
    "            y = y_train_shuffled[begin:end]\n",
    "\n",
    "            # Forward\n",
    "            output = self.feed_forward(x)\n",
    "            # Backprop\n",
    "            grad = self.back_propagate(y, output)\n",
    "            # Optimize\n",
    "            self.optimize(l_rate=l_rate, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9839b-7df6-482b-88c7-f8e7058fbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(self, l_rate=0.1, beta=.9):\n",
    "    '''\n",
    "        Stochatic Gradient Descent (SGD):\n",
    "        θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "        Momentum:\n",
    "        v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "        θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "    '''\n",
    "    if self.optimizer == \"sgd\":\n",
    "        for key in self.params:\n",
    "            self.params[key] = self.params[key] -\\\n",
    "                                        l_rate*self.grads[key]\n",
    "    elif self.optimizer == \"momentum\":\n",
    "        for key in self.params:\n",
    "            self.momemtum_opt[key] = (beta*self.momemtum_opt[key] +\\\n",
    "                                      (1.-beta)*self.grads[key])\n",
    "            self.params[key] = self.params[key] -\\\n",
    "                                        l_rate * self.momemtum_opt[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012ccf9-80fb-416f-a20c-4af9005473ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(self, y, output):\n",
    "    return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae6de7-a51d-42ce-933d-4257b1ffefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9653e48-4136-493b-bb47-c5ccaddd2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, activation='sigmoid'):\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        else:\n",
    "            raise ValueError(\"Activation function is currently not support, please use 'relu' or 'sigmoid' instead.\")\n",
    "        \n",
    "        # Save all weights\n",
    "        self.params = self.initialize()\n",
    "        # Save all intermediate values, i.e. activations\n",
    "        self.cache = {}\n",
    "        \n",
    "    def relu(self, x, derivative=False):\n",
    "        '''\n",
    "            Derivative of ReLU is a bit more complicated since it is not differentiable at x = 0\n",
    "        \n",
    "            Forward path:\n",
    "            relu(x) = max(0, x)\n",
    "            In other word,\n",
    "            relu(x) = 0, if x < 0\n",
    "                    = x, if x >= 0\n",
    "\n",
    "            Backward path:\n",
    "            ∇relu(x) = 0, if x < 0\n",
    "                     = 1, if x >=0\n",
    "        '''\n",
    "        if derivative:\n",
    "            x = np.where(x < 0, 0, x)\n",
    "            x = np.where(x >= 0, 1, x)\n",
    "            return x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            σ(x) = 1 / 1+exp(-z)\n",
    "            \n",
    "            Backward path:\n",
    "            ∇σ(x) = exp(-z) / (1+exp(-z))^2\n",
    "        '''\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''\n",
    "            softmax(x) = exp(x) / ∑exp(x)\n",
    "        '''\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialize(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_layer=self.sizes[1]\n",
    "        output_layer=self.sizes[2]\n",
    "        \n",
    "        params = {\n",
    "            \"W1\": np.random.randn(hidden_layer, input_layer) * np.sqrt(1./input_layer),\n",
    "            \"b1\": np.zeros((hidden_layer, 1)) * np.sqrt(1./input_layer),\n",
    "            \"W2\": np.random.randn(output_layer, hidden_layer) * np.sqrt(1./hidden_layer),\n",
    "            \"b2\": np.zeros((output_layer, 1)) * np.sqrt(1./hidden_layer)\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    def initialize_momemtum_optimizer(self):\n",
    "        momemtum_opt = {\n",
    "            \"W1\": np.zeros(self.params[\"W1\"].shape),\n",
    "            \"b1\": np.zeros(self.params[\"b1\"].shape),\n",
    "            \"W2\": np.zeros(self.params[\"W2\"].shape),\n",
    "            \"b2\": np.zeros(self.params[\"b2\"].shape),\n",
    "        }\n",
    "        return momemtum_opt\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        '''\n",
    "            y = σ(wX + b)\n",
    "        '''\n",
    "        self.cache[\"X\"] = x\n",
    "        self.cache[\"Z1\"] = np.matmul(self.params[\"W1\"], self.cache[\"X\"].T) + self.params[\"b1\"]\n",
    "        self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
    "        self.cache[\"Z2\"] = np.matmul(self.params[\"W2\"], self.cache[\"A1\"]) + self.params[\"b2\"]\n",
    "        self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"])\n",
    "        return self.cache[\"A2\"]\n",
    "    \n",
    "    def back_propagate(self, y, output):\n",
    "        '''\n",
    "            This is the backpropagation algorithm, for calculating the updates\n",
    "            of the neural network's parameters.\n",
    "\n",
    "            Note: There is a stability issue that causes warnings. This is \n",
    "                  caused  by the dot and multiply operations on the huge arrays.\n",
    "                  \n",
    "                  RuntimeWarning: invalid value encountered in true_divide\n",
    "                  RuntimeWarning: overflow encountered in exp\n",
    "                  RuntimeWarning: overflow encountered in square\n",
    "        '''\n",
    "        current_batch_size = y.shape[0]\n",
    "        \n",
    "        dZ2 = output - y.T\n",
    "        dW2 = (1./current_batch_size) * np.matmul(dZ2, self.cache[\"A1\"].T)\n",
    "        db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.matmul(self.params[\"W2\"].T, dZ2)\n",
    "        dZ1 = dA1 * self.activation(self.cache[\"Z1\"], derivative=True)\n",
    "        dW1 = (1./current_batch_size) * np.matmul(dZ1, self.cache[\"X\"])\n",
    "        db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "        return self.grads\n",
    "    \n",
    "    def cross_entropy_loss(self, y, output):\n",
    "        '''\n",
    "            L(y, ŷ) = −∑ylog(ŷ).\n",
    "        '''\n",
    "        l_sum = np.sum(np.multiply(y.T, np.log(output)))\n",
    "        m = y.shape[0]\n",
    "        l = -(1./m) * l_sum\n",
    "        return l\n",
    "                \n",
    "    def optimize(self, l_rate=0.1, beta=.9):\n",
    "        '''\n",
    "            Stochatic Gradient Descent (SGD):\n",
    "            θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "            \n",
    "            Momentum:\n",
    "            v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "            θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "        '''\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for key in self.params:\n",
    "                self.params[key] = self.params[key] - l_rate * self.grads[key]\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for key in self.params:\n",
    "                self.momemtum_opt[key] = (beta * self.momemtum_opt[key] + (1. - beta) * self.grads[key])\n",
    "                self.params[key] = self.params[key] - l_rate * self.momemtum_opt[key]\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer is currently not support, please use 'sgd' or 'momentum' instead.\")\n",
    "\n",
    "    def accuracy(self, y, output):\n",
    "        return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs=10, \n",
    "              batch_size=64, optimizer='momentum', l_rate=0.1, beta=.9):\n",
    "        # Hyperparameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        num_batches = -(-x_train.shape[0] // self.batch_size)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optimizer\n",
    "        if self.optimizer == 'momentum':\n",
    "            self.momemtum_opt = self.initialize_momemtum_optimizer()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        template = \"Epoch {}: {:.2f}s, train acc={:.2f}, train loss={:.2f}, test acc={:.2f}, test loss={:.2f}\"\n",
    "        \n",
    "        # Train\n",
    "        for i in range(self.epochs):\n",
    "            # Shuffle\n",
    "            permutation = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            for j in range(num_batches):\n",
    "                # Batch\n",
    "                begin = j * self.batch_size\n",
    "                end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "                x = x_train_shuffled[begin:end]\n",
    "                y = y_train_shuffled[begin:end]\n",
    "                \n",
    "                # Forward\n",
    "                output = self.feed_forward(x)\n",
    "                # Backprop\n",
    "                grad = self.back_propagate(y, output)\n",
    "                # Optimize\n",
    "                self.optimize(l_rate=l_rate, beta=beta)\n",
    "\n",
    "            # Evaluate performance\n",
    "            # Training data\n",
    "            output = self.feed_forward(x_train)\n",
    "            train_acc = self.accuracy(y_train, output)\n",
    "            train_loss = self.cross_entropy_loss(y_train, output)\n",
    "            # Test data\n",
    "            output = self.feed_forward(x_test)\n",
    "            test_acc = self.accuracy(y_test, output)\n",
    "            test_loss = self.cross_entropy_loss(y_test, output)\n",
    "            print(template.format(i+1, time.time()-start_time, train_acc, train_loss, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c1aad-6511-459e-bcc1-16d4c8c40420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid + Momentum\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='sigmoid')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='momentum', l_rate=4, beta=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a471e-82a5-45ef-b803-3eaac943f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU + SGD\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='relu')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='sgd', l_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655004e6-6b99-49e2-aaae-2057deaf802e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
